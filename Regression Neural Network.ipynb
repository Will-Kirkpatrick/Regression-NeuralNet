{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.003, loss: 0.496 (data_loss: 0.496, reg_loss: 0.000),lr: 0.005\n",
      "epoch: 1000, acc: 0.936, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000),lr: 0.002501250625312656\n",
      "epoch: 2000, acc: 0.947, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000),lr: 0.0016672224074691564\n",
      "epoch: 3000, acc: 0.951, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000),lr: 0.0012503125781445363\n",
      "epoch: 4000, acc: 0.954, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000),lr: 0.0010002000400080014\n",
      "epoch: 5000, acc: 0.960, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000),lr: 0.0008334722453742291\n",
      "epoch: 6000, acc: 0.956, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000),lr: 0.000714387769681383\n",
      "epoch: 7000, acc: 0.968, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000),lr: 0.0006250781347668457\n",
      "epoch: 8000, acc: 0.972, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000),lr: 0.00055561729081009\n",
      "epoch: 9000, acc: 0.982, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000),lr: 0.0005000500050005\n",
      "epoch: 10000, acc: 0.981, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000),lr: 0.00045458678061641964\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import sine_data\n",
    "import matplotlib.pyplot as plt\n",
    "nnfs.init()\n",
    "\n",
    "class Layer_Dense:\n",
    "    def __init__(self, n_inputs, n_neurons,weight_regularizer_l1=0,weight_regularizer_l2=0,\n",
    "                bias_regularizer_l1=0, bias_regularizer_l2=0):\n",
    "        self.weights = 0.1*np.random.randn(n_inputs,n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "        \n",
    "        self.weight_regularizer_l1 = weight_regularizer_l1\n",
    "        self.weight_regularizer_l2 = weight_regularizer_l2\n",
    "        self.bias_regularizer_l1 = bias_regularizer_l1\n",
    "        self.bias_regularizer_l2 = bias_regularizer_l2\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "    def backward(self,dvalues):\n",
    "        self.dweights = np.dot(self.inputs.T,dvalues)\n",
    "        self.dbiases = np.sum(dvalues,axis=0,keepdims=True)\n",
    "        \n",
    "        if self.weight_regularizer_l1 > 0:\n",
    "            dL1 = np.ones_like(self.weights)\n",
    "            dL1[self.weights < 0] = -1\n",
    "            self.dweights += self.weight_regularizer_l1 *dL1\n",
    "        if self.weight_regularizer_l2 > 0:\n",
    "            self.dweights += 2*self.weight_regularizer_l2 * self.weights\n",
    "            \n",
    "        if self.bias_regularizer_l1 > 0:\n",
    "            dL1 = np.ones_like(self.biases)\n",
    "            dL1[self.biases < 0] = -1\n",
    "            self.dbiases += self.bias_regularizer_l1 * dL1\n",
    "        if self.bias_regularizer_l2 > 0:\n",
    "            self.dbiases += 2*self.bias_regularizer_l2*self.biases\n",
    "\n",
    "        self.dinputs = np.dot(dvalues,self.weights.T)\n",
    "        \n",
    "class Layer_Dropout:\n",
    "    def __init__(self,rate):\n",
    "        self.rate = 1 - rate\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.binary_mask = np.random.binomial(1,self.rate, size=inputs.shape)/self.rate\n",
    "        self.output = inputs * self.binary_mask\n",
    "    def backward(self,dvalues):\n",
    "        self.dinputs = dvalues*self.binary_mask\n",
    "\n",
    "'''activation functions'''\n",
    "class Activation_ReLU:\n",
    "    def forward(self,inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.maximum(0,inputs)\n",
    "    def backward(self,dvalues):\n",
    "        self.dinputs = dvalues.copy()\n",
    "        self.dinputs[self.inputs <= 0] = 0\n",
    "        \n",
    "class Activation_Sigmoid:\n",
    "    def forward(self,inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = 1/(1+np.exp(-inputs))\n",
    "    def backward(self,dvalues):\n",
    "        self.dinputs = dvalues*(1-self.output)*self.output\n",
    "        \n",
    "class Activation_Softmax:\n",
    "    def forward(self,inputs):\n",
    "        self.inputs = inputs\n",
    "        exp_values=np.exp(inputs-np.max(inputs,axis=1,keepdims=True))\n",
    "        probabilities = exp_values/np.sum(exp_values,axis=1,keepdims=True)\n",
    "        self.output = probabilities\n",
    "    def backward(self,dvalues):\n",
    "        self.dinputs = np.empty_like(dvalues) #create empty array\n",
    "        for index, (single_output,single_dvalues) in enumerate(zip(self.output,dvalues)):\n",
    "            single_ouput = single_output.reshape(-1,1)\n",
    "            jacobian_matrix = np.diagflat(single_ouput) - np.dot(single_output, single_output.T)\n",
    "            self.dinputs[index] = np.dot(jacobian_matrix,single_dvalues)\n",
    "            \n",
    "class Activation_Linear:\n",
    "    def forward(self,inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = inputs\n",
    "    def backward(self,dvalues):\n",
    "        self.dinputs = dvalues.copy()\n",
    "\n",
    "'''optimizers'''          \n",
    "class Optimizer_SGD:\n",
    "    def __init__(self,learning_rate=1.,decay=0.,momentum=0.):\n",
    "        self.learning_rate=learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.momentum = momentum\n",
    "        \n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1/(1+self.decay*self.iterations))\n",
    "            \n",
    "    def update_params(self,layer):\n",
    "        if self.momentum:\n",
    "            if not hasattr(layer, 'weight_momentums'):\n",
    "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "                layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "            weight_updates = self.momentum*layer.weight_momentums - self.current_learning_rate*layer.dweights\n",
    "            layer.weight_momentums = weight_updates\n",
    "            bias_updates = self.momentum*layer.bias_momentums - self.current_learning_rate*layer.dbiases\n",
    "            layer.bias_momentums = bias_updates\n",
    "        else:\n",
    "            weight_updates = -self.current_learning_rate*layer.dweights\n",
    "            bias_updates = -self.current_learning_rate*layer.dbiases\n",
    "        layer.weights += weight_updates\n",
    "        layer.biases += bias_updates\n",
    "        \n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "        \n",
    "class Optimizer_Adagrad:\n",
    "    def __init__(self,learning_rate=1.,decay=0.,epsilon=1e-7):\n",
    "        self.learning_rate=learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1/(1+self.decay*self.iterations))\n",
    "            \n",
    "    def update_params(self,layer):\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "        layer.weight_cache += layer.dweights**2\n",
    "        layer.bias_cache += layer.dbiases**2            \n",
    "        layer.weights += -self.current_learning_rate*layer.dweights/(np.sqrt(layer.weight_cache)+self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate*layer.dbiases/(np.sqrt(layer.bias_cache)+self.epsilon)\n",
    "        \n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "class Optimizer_RMSprop:\n",
    "    def __init__(self,learning_rate=0.001,decay=0.,epsilon=1e-7,rho=0.9):\n",
    "        self.learning_rate=learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.rho = rho\n",
    "        \n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1./(1.+self.decay*self.iterations))\n",
    "            \n",
    "    def update_params(self,layer):\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "        layer.weight_cache = self.rho*layer.weight_cache + (1-self.rho)*layer.dweights**2\n",
    "        layer.bias_cache = self.rho*layer.bias_cache + (1-self.rho)*layer.dbiases**2            \n",
    "        layer.weights += -self.current_learning_rate*layer.dweights/(np.sqrt(layer.weight_cache)+self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate*layer.dbiases/(np.sqrt(layer.bias_cache)+self.epsilon)\n",
    "        \n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "        \n",
    "class Optimizer_Adam:\n",
    "    def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7, beta_1=0.9, beta_2=0.999):\n",
    "        self.learning_rate=learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "        \n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1/(1+self.decay*self.iterations))\n",
    "            \n",
    "    def update_params(self,layer):\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "        \n",
    "        layer.weight_momentums = self.beta_1*layer.weight_momentums + (1-self.beta_1)*layer.dweights\n",
    "        layer.bias_momentums = self.beta_1*layer.bias_momentums + (1-self.beta_1)*layer.dbiases\n",
    "        \n",
    "        weight_momentums_corrected = layer.weight_momentums/(1-self.beta_1**(self.iterations+1))\n",
    "        bias_momentums_corrected = layer.bias_momentums/(1-self.beta_1**(self.iterations+1))\n",
    "        \n",
    "        layer.weight_cache = self.beta_2*layer.weight_cache + (1-self.beta_2)*layer.dweights**2\n",
    "        layer.bias_cache = self.beta_2*layer.bias_cache + (1-self.beta_2)*layer.dbiases**2     \n",
    "        weight_cache_corrected = layer.weight_cache/(1-self.beta_2**(self.iterations+1))\n",
    "        bias_cache_corrected = layer.bias_cache/(1-self.beta_2**(self.iterations+1))\n",
    "        \n",
    "        layer.weights += -self.current_learning_rate*weight_momentums_corrected/(np.sqrt(weight_cache_corrected)+self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate*bias_momentums_corrected/(np.sqrt(bias_cache_corrected)+self.epsilon)\n",
    "        \n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "            \n",
    "            \n",
    "'''loss functions'''                      \n",
    "class Loss:\n",
    "    def regularization_loss(self,layer):\n",
    "        regularization_loss = 0\n",
    "        if layer.weight_regularizer_l1 > 0:\n",
    "            regularization_loss += layer.weight_regularizer_l1 * np.sum(np.abs(layer.weights))\n",
    "        if layer.weight_regularizer_l2 > 0:\n",
    "            regularization_loss += layer.weight_regularizer_l2 * np.sum(layer.weights**2)\n",
    "        if layer.bias_regularizer_l1 > 0:\n",
    "            regularization_loss += layer.bias_regularizer_l1 * np.sum(np.abs(layer.biases))\n",
    "        if layer.bias_regularizer_l2 > 0:\n",
    "            regularization_loss += layer.bias_regularizer_l2 * np.sum(layer.biases**2)\n",
    "        return regularization_loss\n",
    "        \n",
    "        \n",
    "    def calculate(self,output,y):\n",
    "        sample_losses = self.forward(output,y)\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        return data_loss\n",
    "    \n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "    def forward(self,y_pred,y_true):\n",
    "        samples = len(y_pred)\n",
    "        y_pred_clipped = np.clip(y_pred,1e-7,1-1e-7)\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[range(samples),y_true]\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(y_pred_clipped*y_true,axis=1)\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "    \n",
    "    def backward(self,dvalues,y_true):\n",
    "        samples = len(dvalues)\n",
    "        labels = len(dvalues[0])\n",
    "        #if labels are sparse, turn them into one-hot vector\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(labels)[y_true] #creates matrix with 1s along the diagonal\n",
    "        self.dinputs = -y_true/dvalues #calculate gradients\n",
    "        self.dinputs = self.dinputs/samples #normalise    \n",
    "        \n",
    "class Activation_Softmax_Loss_CategoricalCrossentropy():\n",
    "    def __init__(self):\n",
    "        self.activation = Activation_Softmax()\n",
    "        self.loss = Loss_CategoricalCrossentropy()\n",
    "    def forward(self,inputs,y_true):\n",
    "        self.activation.forward(inputs)\n",
    "        self.output = self.activation.output\n",
    "        return self.loss.calculate(self.output,y_true)\n",
    "    def backward(self,dvalues,y_true):\n",
    "        samples = len(dvalues)\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true,axis=1)\n",
    "        self.dinputs = dvalues.copy()\n",
    "        self.dinputs[range(samples),y_true] -= 1\n",
    "        self.dinputs = self.dinputs/samples\n",
    "                       \n",
    "class Loss_BinaryCrossentropy(Loss):\n",
    "    def forward(self,y_pred,y_true):\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "        sample_losses = -(y_true*np.log(y_pred_clipped)+(1 - y_true)*np.log(1 - y_pred_clipped))\n",
    "        sample_losses = np.mean(sample_losses, axis=-1)\n",
    "        return sample_losses\n",
    "    def backward(self,dvalues,y_true):\n",
    "        samples = len(dvalues)\n",
    "        outputs = len(dvalues[0])\n",
    "        clipped_dvalues = np.clip(dvalues, 1e-7, 1 - 1e-7)\n",
    "        \n",
    "        self.dinputs = -(y_true/clipped_dvalues - (1 - y_true)/(1 - clipped_dvalues)) / outputs\n",
    "        self.dinputs = self.dinputs/samples\n",
    "        \n",
    "class Loss_MeanSquaredError(Loss):\n",
    "    def forward(self, y_pred, y_true):\n",
    "        sample_losses = np.mean((y_true - y_pred)**2, axis=-1)\n",
    "        return sample_losses\n",
    "    def backward(self,dvalues,y_true):\n",
    "        samples = len(dvalues)\n",
    "        outputs = len(dvalues[0])\n",
    "        self.dinputs = -2*(y_true - dvalues) / outputs\n",
    "        self.dinputs = self.dinputs / samples\n",
    "        \n",
    "class Loss_MeanAbsoluteError(Loss):\n",
    "    def forward(self,y_pred,y_true):\n",
    "        sample_losses= np.mean(np.abs(y_true - y_pred), axis=-1)\n",
    "        return sample_losses\n",
    "    def backward(self,dvalues,y_true):\n",
    "        samples = len(dvalues)\n",
    "        outputs = len(dvalues[0])\n",
    "        self.dinputs = np.sign(y_true - dvalues) / outputs\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "\n",
    "X,y=sine_data()\n",
    "\n",
    "dense1 = Layer_Dense(1,64)\n",
    "activation1 = Activation_ReLU()\n",
    "dense2 = Layer_Dense(64,64)\n",
    "activation2 = Activation_ReLU()\n",
    "dense3 = Layer_Dense(64,1)\n",
    "activation3 = Activation_Linear()\n",
    "loss_function = Loss_MeanSquaredError()\n",
    "optimizer = Optimizer_Adam(learning_rate=0.005, decay=1e-3)\n",
    "#calculate accuracy by checking how many values have a difference from ground truth equivalent smaller than given precision\n",
    "accuracy_precision = np.std(y) / 250\n",
    "\n",
    "for epoch in range(10001):\n",
    "    dense1.forward(X)\n",
    "    activation1.forward(dense1.output)\n",
    "    dense2.forward(activation1.output)\n",
    "    activation2.forward(dense2.output)\n",
    "    dense3.forward(activation2.output)\n",
    "    activation3.forward(dense3.output)\n",
    "    data_loss = loss_function.calculate(activation3.output,y)\n",
    "    regularization_loss = loss_function.regularization_loss(dense1) + loss_function.regularization_loss(dense2)\\\n",
    "    + loss_function.regularization_loss(dense3)\n",
    "    loss = data_loss + regularization_loss\n",
    "    \n",
    "    predictions = activation3.output\n",
    "    accuracy = np.mean(np.absolute(predictions-y) < accuracy_precision)\n",
    "\n",
    "    if not epoch % 1000:\n",
    "        print(f'epoch: {epoch}, ' + f'acc: {accuracy:.3f}, ' + f'loss: {loss:.3f} (' +\n",
    "              f'data_loss: {data_loss:.3f}, ' + f'reg_loss: {regularization_loss:.3f}),'+\n",
    "              f'lr: {optimizer.current_learning_rate}')\n",
    "\n",
    "    loss_function.backward(activation3.output,y)\n",
    "    activation3.backward(loss_function.dinputs)\n",
    "    dense3.backward(activation3.dinputs)\n",
    "    activation2.backward(dense3.dinputs)\n",
    "    dense2.backward(activation2.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.update_params(dense3)\n",
    "    optimizer.post_update_params()\n",
    "    \n",
    "        \n",
    "X_test, y_test = sine_data()\n",
    "\n",
    "dense1.forward(X_test)\n",
    "activation1.forward(dense1.output)\n",
    "dense2.forward(activation1.output)\n",
    "activation2.forward(dense2.output)\n",
    "dense3.forward(activation2.output)\n",
    "activation3.forward(dense3.output)\n",
    "\n",
    "plt.plot(X_test,y_test)\n",
    "plt.plot(X_test, activation3.output)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
